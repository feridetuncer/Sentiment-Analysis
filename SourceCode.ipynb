{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcdc3ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from typing import List\n",
    "from jpype import JClass, JString, getDefaultJVMPath, shutdownJVM, startJVM, java\n",
    "import pandas as pd\n",
    "from os.path import join\n",
    "from sklearn.utils import shuffle\n",
    "from pandas import ExcelWriter\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import emoji\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence,text\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report,f1_score,recall_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation,Flatten\n",
    "from keras.layers import  Embedding, SimpleRNN, LSTM,Masking,Bidirectional\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.models import model_from_json,model_from_yaml,load_model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence,text\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import metrics, regularizers\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import backend as K\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fecec62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(string):\n",
    "    return emoji.get_emoji_regexp().sub(u' ', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8722de94",
   "metadata": {},
   "outputs": [],
   "source": [
    "ZEMBEREK_PATH = \"./bin/zemberek-full.jar\"\n",
    "\n",
    "startJVM(getDefaultJVMPath(), '-ea', '-Djava.class.path=%s' % (ZEMBEREK_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a699812b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dosya = ['Trendyol.xlsx','HepsiBurada.xlsx','GittiGidiyor.xlsx']\n",
    "\n",
    "for item in dosya:\n",
    "    data = pd.read_excel(item)\n",
    "\n",
    "    # tüm harfler küçüğe dönüştülür\n",
    "    data = data.apply(lambda x: x.astype(str).str.lower())\n",
    "\n",
    "    # Regex işlemleri\n",
    "    for i in range(len(data)):\n",
    "\n",
    "        # https://t.co/osUgGrOJSz kelimeleri silme\n",
    "        output = re.sub(r\"http[s]?:(?:[a-zA-Z]|[0-9]|[$-_@.& +]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\", \"\",\n",
    "                        data['tweet'][i])\n",
    "\n",
    "        # @kullanıcı adlarını ve etiketleri silme\n",
    "        output = re.sub(r\"\\@\\w*\\b\", \"\", output)\n",
    "        output = re.sub(r\"\\#\\w*\\b\", \"\", output)\n",
    "        # Rakamları temizleme\n",
    "        output = re.sub(r\"\\b\\d+\", \"\", output)\n",
    "\n",
    "        # 3 karakterden az olanları silme\n",
    "        output = re.sub(r\"\\W*\\b\\w{1,2}\\b\", \"\", output)\n",
    "        \n",
    "        #emoji kaldırma\n",
    "        output=remove_emoji(output)\n",
    "\n",
    "        data['tweet'][i] = output\n",
    "\n",
    "    ## Karakterleri temizleme\n",
    "    data['tweet'] = data['tweet'].str.findall('\\w{2,}').str.join(' ')\n",
    "\n",
    "    ## Kelime düzeltme işlemi\n",
    "    TurkishMorphology: JClass = JClass('zemberek.morphology.TurkishMorphology')\n",
    "    TurkishSentenceNormalizer: JClass = JClass(\n",
    "        'zemberek.normalization.TurkishSentenceNormalizer'\n",
    "    )\n",
    "    Paths: JClass = JClass('java.nio.file.Paths')\n",
    "\n",
    "    normalizer = TurkishSentenceNormalizer(\n",
    "        TurkishMorphology.createWithDefaults(),\n",
    "        Paths.get(join('data', 'normalization')),\n",
    "        Paths.get(join('data', 'lm', 'lm.2gram.slm'))\n",
    "    )\n",
    "\n",
    "    for num in range(len(data.tweet)):\n",
    "        # print((\n",
    "        #    f'\\nNormal      : {data.tweet[num]}'\n",
    "        #    f'\\nDuzenlenmiş : {normalizer.normalize(JString(data.tweet[num]))}'\n",
    "        #\n",
    "        #        ))\n",
    "        data.xs(num)['tweet'] = normalizer.normalize(JString(data.tweet[num]))\n",
    "\n",
    "    ## Kelime köklerini bulma ve anlamsızlar UNK yazar\n",
    "    \n",
    "    TurkishMorphology = JClass('zemberek.morphology.TurkishMorphology')\n",
    "    morphology = TurkishMorphology.createWithDefaults()\n",
    "    \n",
    "    for num in range(len(data.tweet)):\n",
    "       # print(\"Orjinal tweet --->\", data.tweet[num])\n",
    "        analysis: java.util.ArrayList = (morphology.analyzeAndDisambiguate(data.tweet[num]).bestAnalysis())\n",
    "        pos: List[str] = []\n",
    "        for i, analysis in enumerate(analysis, start=1):\n",
    "            f'\\nAnalysis {i}: {analysis}', f'\\nPrimary POS {i}: {analysis.getPos()}' f'\\nPrimary POS (Short Form) {i}: {analysis.getPos().shortForm}'\n",
    "            pos.append(\n",
    "                f'{str(analysis.getLemmas()[0])}'\n",
    "            )\n",
    "        data.xs(num)['tweet'] = \" \".join(pos)\n",
    "       # print(f'Islem Gormus  ---> {\" \".join(pos)}',\"\\n\")\n",
    "    \n",
    "    data['tweet']=data['tweet'].str.replace(\"UNK\",\" \")\n",
    "    #data['tweet']=data['tweet'].str.findall('\\w{UNK}').str.join(' ')\n",
    "    \n",
    "\n",
    "    stop_words = stopwords.words('turkish')\n",
    "    with open('turkce-stop-words.txt', encoding='utf-8') as file:\n",
    "        stw = file.read()\n",
    "    stw = stw.split()\n",
    "    stw = [s.lower() for s in stw]\n",
    "    stop_words += stw\n",
    "\n",
    "    data['tweet'] = data['tweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "\n",
    "    data = shuffle(data)\n",
    "\n",
    "    ad = 'temizlendi' + item\n",
    "    writer = ExcelWriter(ad)\n",
    "    data.to_excel(writer, item)\n",
    "    writer.save()\n",
    "\n",
    "shutdownJVM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dac0cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename=\"hepsi\"\n",
    "data = pd.read_excel('./'+filename+'.xlsx')\n",
    "data.dropna( inplace = True)\n",
    "duplicateDFRow = data[data.duplicated(['tweet'])]\n",
    "data.drop_duplicates(subset =\"tweet\",keep = False, inplace = True)\n",
    "data = data.reset_index(drop=True)\n",
    "tags = data[\"tags\"]\n",
    "texts = data[\"tweet\"]\n",
    "data = shuffle(data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb64f7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tagsleri 0 ve 1 yapma (Kategorik verileri nümerik hale getirme)\n",
    "le = LabelEncoder()\n",
    "tags = le.fit_transform(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9240e73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kelime numalarandırmada en büyük sayı\n",
    "num_max = 10000\n",
    "## Kelime numaralandırma işlemi\n",
    "tok = Tokenizer(num_words=num_max)\n",
    "tok.fit_on_texts(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08498d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# En uzun tweet ve farklı kelime sayısı\n",
    "max_len = 15\n",
    "vocab_size=10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd60a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kelime numaralandırma örneği\n",
    "print(texts[10])\n",
    "for item in texts[10].split():    \n",
    "    print(tok.word_index[item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567d9899",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tüm tweetleri numaralandırma\n",
    "cnn_texts_seq = tok.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9f7342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numaralandırılmış tweetleri belirli bir uzunluğa getirme ve padding işlemi\n",
    "cnn_texts_mat = tf.keras.preprocessing.sequence.pad_sequences(cnn_texts_seq,maxlen=max_len,padding='post')\n",
    "\n",
    "# Örnek\n",
    "print('***************************************************')\n",
    "print(texts[40])\n",
    "print(cnn_texts_mat[40])\n",
    "print('***************************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333b28e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_trendyol=pd.read_excel(\"temizlendiTrendyol.xlsx\")\n",
    "h=dataset_trendyol['tags']=='negatif'\n",
    "print(\"Negatif count\" ,len(dataset_trendyol[h]))\n",
    "h=dataset_trendyol['tags']=='pozitif'\n",
    "print(\"Pozitif count\" ,len(dataset_trendyol[h]))\n",
    "dataset_trendyol.iloc[:,1] = le.fit_transform(dataset_trendyol.iloc[:,1])\n",
    "X_train_t,X_test_t,y_train_t,y_test_t=train_test_split(cnn_texts_mat,tags,test_size=0.2)\n",
    "dataset_trendyol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e82ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_hepsiburada=pd.read_excel(\"temizlendiHepsiBurada.xlsx\")\n",
    "h=dataset_hepsiburada['tags']=='negatif'\n",
    "print(\"Negatif count\" ,len(dataset_hepsiburada[h]))\n",
    "h=dataset_hepsiburada['tags']=='pozitif'\n",
    "print(\"Pozitif count\" ,len(dataset_hepsiburada[h]))\n",
    "dataset_hepsiburada.iloc[:,1] = le.fit_transform(dataset_hepsiburada.iloc[:,1])\n",
    "X_train_h,X_test_h,y_train_h,y_test_h=train_test_split(cnn_texts_mat,tags,test_size=0.2)\n",
    "dataset_hepsiburada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de58d23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_gittigidiyor=pd.read_excel(\"temizlendiGittiGidiyor.xlsx\")\n",
    "h=dataset_gittigidiyor['tags']=='negatif'\n",
    "print(\"Negatif count\" ,len(dataset_gittigidiyor[h]))\n",
    "h=dataset_gittigidiyor['tags']=='pozitif'\n",
    "print(\"Pozitif count\" ,len(dataset_gittigidiyor[h]))\n",
    "dataset_gittigidiyor.iloc[:,1] = le.fit_transform(dataset_gittigidiyor.iloc[:,1])\n",
    "X_train_g,X_test_g,y_train_g,y_test_g=train_test_split(cnn_texts_mat,tags,test_size=0.2)\n",
    "dataset_gittigidiyor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea79b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_all=pd.read_excel(\"hepsi.xlsx\")\n",
    "h=dataset_all['tags']=='negatif'\n",
    "print(\"Negatif count\" ,len(dataset_all[h]))\n",
    "h=dataset_all['tags']=='pozitif'\n",
    "print(\"Pozitif count\" ,len(dataset_all[h]))\n",
    "dataset_all.iloc[:,1] = le.fit_transform(dataset_all.iloc[:,1])\n",
    "X_train_a,X_test_a,y_train_a,y_test_a=train_test_split(cnn_texts_mat,tags,test_size=0.2)\n",
    "dataset_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6800316f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#LSTM - Trendyol\n",
    "model=Sequential()\n",
    "model.add(Embedding(vocab_size,max_len, trainable=True,input_length=max_len))\n",
    "model.add(LSTM(32,activation='relu',kernel_regularizer=regularizers.l2(0.01),return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(16,activation='relu',kernel_regularizer=regularizers.l2(0.01),return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(4,activation='relu',kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',optimizer=RMSprop(lr=0.001),metrics=['accuracy'])\n",
    "history_t_l=model.fit(X_train_t,y_train_t, epochs=10, batch_size=8,\n",
    "                      verbose=1,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c92b9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(X_train_t,y_train_t,batch_size=1)\n",
    "print(\"Training Accuracy: %.2f%%\\n\" % (scores[1]*100))\n",
    "scores = model.evaluate(X_test_t,y_test_t,batch_size=1)\n",
    "print(\"Testing Accuracy: %.2f%%\\n\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741d2f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = model.predict(X_test_t)\n",
    "y_pred=[]\n",
    "for i in y_pred_test:\n",
    "    if(i>=0.5):\n",
    "        y_pred.append(1)\n",
    "    else:\n",
    "        y_pred.append(0)\n",
    "cm = confusion_matrix(y_test_t,y_pred)\n",
    "ax = sns.heatmap(cm, annot=True, xticklabels=[\"Pozitif\",'Negatif'], yticklabels=['Pozitif','Negatif'],\n",
    "                cbar=False,cmap='Blues')\n",
    "ax.set_xlabel('Prediction')\n",
    "ax.set_ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69727b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_t, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0a3cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM - HepsiBurada\n",
    "model=Sequential()\n",
    "model.add(Embedding(vocab_size,max_len, trainable=True,input_length=max_len))\n",
    "model.add(LSTM(32,activation='relu',kernel_regularizer=regularizers.l2(0.01),return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(16,activation='relu',kernel_regularizer=regularizers.l2(0.01),return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(4,activation='relu',kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',optimizer=RMSprop(lr=0.001),metrics=['acc'])\n",
    "history_h_l=model.fit(X_train_h,y_train_h, epochs=10, batch_size=8,\n",
    "                      verbose=1,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bdc464",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(X_train_h,y_train_h,batch_size=1)\n",
    "print(\"Training Accuracy: %.2f%%\\n\" % (scores[1]*100))\n",
    "scores = model.evaluate(X_test_h,y_test_h,batch_size=1)\n",
    "print(\"Testing Accuracy: %.2f%%\\n\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d30d1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = model.predict(X_test_h)\n",
    "y_pred=[]\n",
    "for i in y_pred_test:\n",
    "    if(i>=0.5):\n",
    "        y_pred.append(1)\n",
    "    else:\n",
    "        y_pred.append(0)\n",
    "cm = confusion_matrix(y_test_h,y_pred)\n",
    "ax = sns.heatmap(cm, annot=True, xticklabels=[\"Pozitif\",'Negatif'], yticklabels=['Pozitif','Negatif'],\n",
    "                cbar=False,cmap='Blues')\n",
    "ax.set_xlabel('Prediction')\n",
    "ax.set_ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaf05bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_h, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd04869d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM - GittiGidiyor\n",
    "model=Sequential()\n",
    "model.add(Embedding(vocab_size,max_len, trainable=True,input_length=max_len))\n",
    "model.add(LSTM(32,activation='relu',kernel_regularizer=regularizers.l2(0.01),return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(16,activation='relu',kernel_regularizer=regularizers.l2(0.01),return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(4,activation='relu',kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',optimizer=RMSprop(lr=0.001),metrics=['acc'])\n",
    "history_g_l=model.fit(X_train_g,y_train_g, epochs=10, batch_size=8,\n",
    "                      verbose=1,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626a1e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(X_train_g,y_train_g,batch_size=1)\n",
    "print(\"Training Accuracy: %.2f%%\\n\" % (scores[1]*100))\n",
    "scores = model.evaluate(X_test_g,y_test_g,batch_size=1)\n",
    "print(\"Testing Accuracy: %.2f%%\\n\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507d209a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = model.predict(X_test_g)\n",
    "y_pred=[]\n",
    "for i in y_pred_test:\n",
    "    if(i>=0.5):\n",
    "        y_pred.append(1)\n",
    "    else:\n",
    "        y_pred.append(0)\n",
    "cm = confusion_matrix(y_test_g,y_pred)\n",
    "ax = sns.heatmap(cm, annot=True, xticklabels=[\"Pozitif\",'Negatif'], yticklabels=['Pozitif','Negatif'],\n",
    "                cbar=False,cmap='Blues')\n",
    "ax.set_xlabel('Prediction')\n",
    "ax.set_ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900eac6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_g, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211f0baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes - Trendyol\n",
    "classifier = GaussianNB()\n",
    "history_nb_t = classifier.fit(X_train_t, y_train_t)\n",
    "y_pred_nb = classifier.predict(X_test_t)\n",
    "scores = accuracy_score(y_test_t, y_pred_nb)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459e20ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = classifier.predict(X_test_t)\n",
    "y_pred=[]\n",
    "for i in y_pred_test:\n",
    "    if(i>=0.5):\n",
    "        y_pred.append(1)\n",
    "    else:\n",
    "        y_pred.append(0)\n",
    "cm = confusion_matrix(y_test_t,y_pred)\n",
    "ax = sns.heatmap(cm, annot=True, xticklabels=[\"Pozitif\",'Negatif'], yticklabels=['Pozitif','Negatif'],\n",
    "                cbar=False,cmap='Blues')\n",
    "ax.set_xlabel('Prediction')\n",
    "ax.set_ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000fe7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_t, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46320f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes - HepsiBurada\n",
    "classifier = GaussianNB()\n",
    "history_nb_t = classifier.fit(X_train_h, y_train_h)\n",
    "y_pred_nb = classifier.predict(X_test_h)\n",
    "scores = accuracy_score(y_test_h, y_pred_nb)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ab8dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = classifier.predict(X_test_h)\n",
    "y_pred=[]\n",
    "for i in y_pred_test:\n",
    "    if(i>=0.5):\n",
    "        y_pred.append(1)\n",
    "    else:\n",
    "        y_pred.append(0)\n",
    "cm = confusion_matrix(y_test_h,y_pred)\n",
    "ax = sns.heatmap(cm, annot=True, xticklabels=[\"Pozitif\",'Negatif'], yticklabels=['Pozitif','Negatif'],\n",
    "                cbar=False,cmap='Blues')\n",
    "ax.set_xlabel('Prediction')\n",
    "ax.set_ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa088cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_h, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93918c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes - GittiGidiyor\n",
    "classifier = GaussianNB()\n",
    "history_nb_t = classifier.fit(X_train_g, y_train_g)\n",
    "y_pred_nb = classifier.predict(X_test_g)\n",
    "scores = accuracy_score(y_test_g, y_pred_nb)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d21bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = classifier.predict(X_test_g)\n",
    "y_pred=[]\n",
    "for i in y_pred_test:\n",
    "    if(i>=0.5):\n",
    "        y_pred.append(1)\n",
    "    else:\n",
    "        y_pred.append(0)\n",
    "cm = confusion_matrix(y_test_g,y_pred)\n",
    "ax = sns.heatmap(cm, annot=True, xticklabels=[\"Pozitif\",'Negatif'], yticklabels=['Pozitif','Negatif'],\n",
    "                cbar=False,cmap='Blues')\n",
    "ax.set_xlabel('Prediction')\n",
    "ax.set_ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb14e269",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_g, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21fce66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest - Trendyol\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth = 3, random_state=32)\n",
    "rf.fit(X_train_t, y_train_t)\n",
    "predictions = rf.predict(X_test_t)\n",
    "score = round(accuracy_score(y_test_t, predictions), 3)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff927796",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = rf.predict(X_test_t)\n",
    "y_pred=[]\n",
    "for i in y_pred_test:\n",
    "    if(i>=0.5):\n",
    "        y_pred.append(1)\n",
    "    else:\n",
    "        y_pred.append(0)\n",
    "cm = confusion_matrix(y_test_t,y_pred)\n",
    "ax = sns.heatmap(cm, annot=True, xticklabels=[\"Pozitif\",'Negatif'], yticklabels=['Pozitif','Negatif'],\n",
    "                cbar=False,cmap='Blues')\n",
    "ax.set_xlabel('Prediction')\n",
    "ax.set_ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77a6621",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_t, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5a45bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest - HepsiBurada\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth = 3, random_state=32)\n",
    "rf.fit(X_train_h, y_train_h)\n",
    "predictions = rf.predict(X_test_h)\n",
    "score = round(accuracy_score(y_test_h, predictions), 3)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c1a831",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = rf.predict(X_test_h)\n",
    "y_pred=[]\n",
    "for i in y_pred_test:\n",
    "    if(i>=0.5):\n",
    "        y_pred.append(1)\n",
    "    else:\n",
    "        y_pred.append(0)\n",
    "cm = confusion_matrix(y_test_h,y_pred)\n",
    "ax = sns.heatmap(cm, annot=True, xticklabels=[\"Pozitif\",'Negatif'], yticklabels=['Pozitif','Negatif'],\n",
    "                cbar=False,cmap='Blues')\n",
    "ax.set_xlabel('Prediction')\n",
    "ax.set_ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ac5999",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_h, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0babe75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest - GittiGidiyor\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth = 3, random_state=32)\n",
    "rf.fit(X_train_g, y_train_g)\n",
    "predictions = rf.predict(X_test_g)\n",
    "score = round(accuracy_score(y_test_g, predictions), 3)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0baf01e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = rf.predict(X_test_g)\n",
    "y_pred=[]\n",
    "for i in y_pred_test:\n",
    "    if(i>=0.5):\n",
    "        y_pred.append(1)\n",
    "    else:\n",
    "        y_pred.append(0)\n",
    "cm = confusion_matrix(y_test_g,y_pred)\n",
    "ax = sns.heatmap(cm, annot=True, xticklabels=[\"Pozitif\",'Negatif'], yticklabels=['Pozitif','Negatif'],\n",
    "                cbar=False,cmap='Blues')\n",
    "ax.set_xlabel('Prediction')\n",
    "ax.set_ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4454b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_g, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e133e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tüm veri seti - LSTM\n",
    "model=Sequential()\n",
    "model.add(Embedding(vocab_size,max_len, trainable=True,input_length=max_len))\n",
    "model.add(LSTM(32,activation='relu',kernel_regularizer=regularizers.l2(0.01),return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(16,activation='relu',kernel_regularizer=regularizers.l2(0.01),return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(4,activation='relu',kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',optimizer=RMSprop(lr=0.001),metrics=['accuracy'])\n",
    "history_a_l=model.fit(X_train_a,y_train_a, epochs=10, batch_size=8,\n",
    "                      verbose=1,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a97947b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores = model.evaluate(X_train_a,y_train_a,batch_size=1)\n",
    "print(\"Training Accuracy: %.2f%%\\n\" % (scores[1]*100))\n",
    "scores = model.evaluate(X_test_a,y_test_a,batch_size=1)\n",
    "print(\"Testing Accuracy: %.2f%%\\n\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c44e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = model.predict(X_test_a)\n",
    "y_pred=[]\n",
    "for i in y_pred_test:\n",
    "    if(i>=0.5):\n",
    "        y_pred.append(1)\n",
    "    else:\n",
    "        y_pred.append(0)\n",
    "cm = confusion_matrix(y_test_a,y_pred)\n",
    "ax = sns.heatmap(cm, annot=True, xticklabels=[\"Pozitif\",'Negatif'], yticklabels=['Pozitif','Negatif'],\n",
    "                cbar=False,cmap='Blues')\n",
    "ax.set_xlabel('Prediction')\n",
    "ax.set_ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e61f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_a, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65c8f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tüm veri seti - Naive Bayes \n",
    "classifier = GaussianNB()\n",
    "history_nb_t = classifier.fit(X_train_a, y_train_a)\n",
    "y_pred_nb = classifier.predict(X_test_a)\n",
    "scores = accuracy_score(y_test_a, y_pred_nb)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b52c81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred_test = classifier.predict(X_test_a)\n",
    "y_pred=[]\n",
    "for i in y_pred_test:\n",
    "    if(i>=0.5):\n",
    "        y_pred.append(1)\n",
    "    else:\n",
    "        y_pred.append(0)\n",
    "cm = confusion_matrix(y_test_a,y_pred)\n",
    "ax = sns.heatmap(cm, annot=True, xticklabels=[\"Pozitif\",'Negatif'], yticklabels=['Pozitif','Negatif'],\n",
    "                cbar=False,cmap='Blues')\n",
    "ax.set_xlabel('Prediction')\n",
    "ax.set_ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f08033",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_a, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cc080b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tüm veri seti - Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth = 3, random_state=32)\n",
    "rf.fit(X_train_a, y_train_a)\n",
    "predictions = rf.predict(X_test_a)\n",
    "score = round(accuracy_score(y_test_a, predictions), 3)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749e556a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = rf.predict(X_test_a)\n",
    "y_pred=[]\n",
    "for i in y_pred_test:\n",
    "    if(i>=0.5):\n",
    "        y_pred.append(1)\n",
    "    else:\n",
    "        y_pred.append(0)\n",
    "cm = confusion_matrix(y_test_a,y_pred)\n",
    "ax = sns.heatmap(cm, annot=True, xticklabels=[\"Pozitif\",'Negatif'], yticklabels=['Pozitif','Negatif'],\n",
    "                cbar=False,cmap='Blues')\n",
    "ax.set_xlabel('Prediction')\n",
    "ax.set_ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2071ef34",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_a, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6549b06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
